{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "942f9ebf",
   "metadata": {},
   "source": [
    "# Transformation de CV en Dossier de Comp√©tences\n",
    "\n",
    "Ce notebook pr√©sente un pipeline complet pour transformer un CV en dossier de comp√©tences.\n",
    "\n",
    "Les √©tapes comprennent :\n",
    "\n",
    "- **Extraction de texte** : Soit via une extraction native (PDF s√©lectionnable) ou par OCR (PaddleOCR).\n",
    "- **Extraction OCR-Free** : Utilisation du mod√®le Donut pour extraire des informations directement depuis l‚Äôimage.\n",
    "- **Conversion JSON** : Transformation de la sortie JSON en un format lisible par l‚Äôhumain.\n",
    "- **Calcul de similarit√©** : Comparaison des comp√©tences extraites avec un ensemble id√©al.\n",
    "- **Extraction NER** : Utilisation de Camembert pour la reconnaissance d‚Äôentit√©s (extraction de comp√©tences, langues, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "420da2e5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fitz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Imports n√©cessaires\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfitz\u001b[39;00m  \u001b[38;5;66;03m# PyMuPDF\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytesseract\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image, ImageDraw, ImageFont\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fitz'"
     ]
    }
   ],
   "source": [
    "# Imports n√©cessaires\n",
    "import fitz  # PyMuPDF\n",
    "import pytesseract\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "import cv2\n",
    "from langdetect import detect\n",
    "from paddleocr import PaddleOCR\n",
    "import json\n",
    "\n",
    "# Pour l'approche OCR-Free avec Donut\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "# Pour NER avec Camembert\n",
    "from transformers import pipeline, CamembertTokenizer\n",
    "\n",
    "# Configurez le chemin de Tesseract (√† adapter selon votre environnement)\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Users\\mathi\\anaconda3\\Lib\\site-packages\\pytesseract\\tesseract.exe\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6d7510",
   "metadata": {},
   "source": [
    "## 1. D√©tection de la langue d'une image\n",
    "\n",
    "Cette fonction utilise Tesseract pour extraire le texte depuis une image, puis `langdetect` pour d√©terminer la langue.\n",
    "\n",
    "**Exemple toy :** Nous cr√©ons une image simple contenant du texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb62a1d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def detect_language_from_image(image):\n",
    "    # Extraction de texte avec Tesseract (rapide)\n",
    "    text = pytesseract.image_to_string(image, lang=\"eng+fra\")\n",
    "    \n",
    "    if text.strip():\n",
    "        try:\n",
    "            detected_lang = detect(text)\n",
    "            print(f\"üîç Detected Language: {detected_lang}\")  # Debug\n",
    "            if detected_lang.startswith(\"fr\"):\n",
    "                return \"fr\"\n",
    "            elif detected_lang.startswith(\"en\"):\n",
    "                return \"en\"\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur de d√©tection de langue: {e}\")\n",
    "            return \"unknown\"\n",
    "    return \"unknown\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7c7a66",
   "metadata": {},
   "source": [
    "**Toy Example 1 ‚Äì D√©tection de la langue :**\n",
    "\n",
    "Cr√©ons une image contenant du texte en fran√ßais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3231fe67",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Cr√©ation d'une image blanche avec du texte\n",
    "toy_img = Image.new('RGB', (300, 100), color=(255, 255, 255))\n",
    "draw = ImageDraw.Draw(toy_img)\n",
    "# Utilisation d'une police par d√©faut (si disponible)\n",
    "try:\n",
    "    font = ImageFont.truetype(\"arial.ttf\", 20)\n",
    "except IOError:\n",
    "    font = None\n",
    "draw.text((10, 40), \"Bonjour, comment √ßa va ?\", fill=(0, 0, 0), font=font)\n",
    "\n",
    "lang_toy = detect_language_from_image(toy_img)\n",
    "print(\"Langue d√©tect√©e (toy):\", lang_toy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1280bfd",
   "metadata": {},
   "source": [
    "## 2. Extraction de texte via OCR avec PaddleOCR\n",
    "\n",
    "Cette fonction extrait le texte d'un PDF en utilisant PaddleOCR.\n",
    "\n",
    "**Remarque :** Pour cet exemple, un PDF r√©el est n√©cessaire. Ici, nous pr√©sentons la fonction.\n",
    "\n",
    "$$\\text{Exemple : } \\text{extract\\_text\\_with\\_paddleocr(pdf\\_path)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90cb9ab",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extract_text_with_paddleocr(pdf_path):\n",
    "    extracted_text = \"\"\n",
    "    with fitz.open(pdf_path) as pdf:\n",
    "        if len(pdf) == 0:\n",
    "            print(\"Erreur: Le PDF est vide ou corrompu.\")\n",
    "            return \"\"\n",
    "        \n",
    "        for page_num, page in enumerate(pdf, start=1):\n",
    "            zoom = 2.0  # Augmentation de la r√©solution pour l'OCR\n",
    "            mat = fitz.Matrix(zoom, zoom)\n",
    "            pix = page.get_pixmap(matrix=mat, alpha=False)\n",
    "            img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "            \n",
    "            # D√©tection de la langue de la page\n",
    "            lang = detect_language_from_image(img)\n",
    "            \n",
    "            # S√©lection du mod√®le PaddleOCR en fonction de la langue d√©tect√©e\n",
    "            if lang == \"fr\":\n",
    "                print(\"üá´üá∑ Utilisation de PaddleOCR en mode fran√ßais...\")\n",
    "                ocr = PaddleOCR(use_angle_cls=True, lang=\"latin\", use_gpu=False)\n",
    "            else:\n",
    "                print(\"üá¨üáß Utilisation de PaddleOCR en mode anglais...\")\n",
    "                ocr = PaddleOCR(use_angle_cls=True, lang=\"en\", use_gpu=False)\n",
    "            \n",
    "            # Ex√©cution de l'OCR avec PaddleOCR\n",
    "            ocr_result = ocr.ocr(np.array(img), det=True, rec=True, cls=True)\n",
    "            extracted_text += f\"\\n--- Page {page_num} (OCR) ---\\n\"\n",
    "            \n",
    "            if ocr_result and isinstance(ocr_result, list) and ocr_result[0]:\n",
    "                for line in ocr_result[0]:\n",
    "                    if isinstance(line, list) and len(line) > 1:\n",
    "                        text_tuple = line[1]\n",
    "                        if isinstance(text_tuple, tuple) and len(text_tuple) > 0:\n",
    "                            text_value = text_tuple[0]\n",
    "                            if isinstance(text_value, str):\n",
    "                                extracted_text += text_value + \"\\n\"\n",
    "            else:\n",
    "                extracted_text += \"\\n[Pas de texte d√©tect√© sur cette page]\\n\"\n",
    "    return extracted_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c9da27",
   "metadata": {},
   "source": [
    "## 3. Pipeline d'extraction du PDF\n",
    "\n",
    "Cette fonction d√©termine si l'extraction native du texte depuis le PDF (via PyMuPDF) est possible.\n",
    "Si le texte extrait est insuffisant (moins de 100 caract√®res), elle utilise l'OCR avec PaddleOCR.\n",
    "\n",
    "$$\\text{Si } |text| > 100 \\text{ alors extraction native, sinon utilisation de l'OCR.}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3193ee65",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extraction_pipeline(pdf_path):\n",
    "    # Extraction native via PyMuPDF\n",
    "    text_native = \"\"\n",
    "    with fitz.open(pdf_path) as pdf:\n",
    "        for page in pdf:\n",
    "            text_native += page.get_text()\n",
    "    \n",
    "    if len(text_native.strip()) > 100:\n",
    "        print(\"Extraction native r√©ussie, utilisation du texte extrait.\")\n",
    "        return text_native\n",
    "    else:\n",
    "        print(\"Extraction native insuffisante, utilisation de l'OCR.\")\n",
    "        return extract_text_with_paddleocr(pdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21b4779",
   "metadata": {},
   "source": [
    "## 4. Extraction OCR-Free avec Donut\n",
    "\n",
    "Utilisation du mod√®le Donut pour extraire des informations directement depuis l'image, sans passer par un OCR explicite.\n",
    "\n",
    "**√âtapes :**\n",
    "1. Charger le mod√®le et le processor.\n",
    "2. Convertir la premi√®re page du PDF en image.\n",
    "3. Extraire le contenu via Donut.\n",
    "\n",
    "$$\\text{Donut output : une cha√Æne JSON structur√©e.}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc1c230",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Chargement du mod√®le Donut\n",
    "donut_model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "donut_processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "\n",
    "def extract_with_donut(pdf_path):\n",
    "    # Traitement de la premi√®re page du PDF\n",
    "    with fitz.open(pdf_path) as pdf:\n",
    "        page = pdf[0]\n",
    "        zoom = 2.0\n",
    "        mat = fitz.Matrix(zoom, zoom)\n",
    "        pix = page.get_pixmap(matrix=mat, alpha=False)\n",
    "        image = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "    \n",
    "    pixel_values = donut_processor(image, return_tensors=\"pt\").pixel_values\n",
    "    outputs = donut_model.generate(pixel_values)\n",
    "    decoded_output = donut_processor.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return decoded_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fd1b9a",
   "metadata": {},
   "source": [
    "## 5. Conversion du JSON en format lisible\n",
    "\n",
    "Cette fonction prend en entr√©e une cha√Æne JSON (par exemple, la sortie du mod√®le Donut) et la convertit en un texte format√© pour une lecture humaine.\n",
    "\n",
    "$$\\text{Conversion : JSON } \\to \\text{ texte lisible.}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034412b6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def json_to_human_readable(json_output):\n",
    "    try:\n",
    "        data = json.loads(json_output)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Erreur: La sortie n'est pas un JSON valide.\")\n",
    "        return json_output  # Retourne la cha√Æne brute si le JSON n'est pas valide\n",
    "    \n",
    "    # Formatage du JSON de mani√®re lisible\n",
    "    readable = \"\"\n",
    "    # On suppose que le JSON a une structure type dossier de comp√©tences.\n",
    "    if \"skills_portfolio\" in data:\n",
    "        portfolio = data[\"skills_portfolio\"]\n",
    "        if \"personal_info\" in portfolio:\n",
    "            info = portfolio[\"personal_info\"]\n",
    "            readable += f\"Nom: {info.get('name', 'N/A')}\\n\"\n",
    "            readable += f\"R√©sidence: {info.get('residence', 'N/A')}\\n\\n\"\n",
    "        if \"professional_experience\" in portfolio:\n",
    "            readable += \"Exp√©rience professionnelle:\\n\"\n",
    "            for exp in portfolio[\"professional_experience\"]:\n",
    "                readable += f\"- Entreprise: {exp.get('company', 'N/A')}, Dur√©e: {exp.get('duration', 'N/A')}\\n\"\n",
    "                readable += f\"  Contexte: {exp.get('context', '')}\\n\"\n",
    "                readable += f\"  R√©alisations: {exp.get('achievements', '')}\\n\\n\"\n",
    "        if \"education\" in portfolio:\n",
    "            readable += \"Formation:\\n\"\n",
    "            for edu in portfolio[\"education\"]:\n",
    "                readable += f\"- {edu.get('degree', 'N/A')} ({edu.get('period', 'N/A')}) √† {edu.get('school', 'N/A')}\\n\"\n",
    "            readable += \"\\n\"\n",
    "        if \"languages\" in portfolio:\n",
    "            readable += \"Langues:\\n\"\n",
    "            for lang in portfolio[\"languages\"]:\n",
    "                readable += f\"- {lang.get('name', 'N/A')} ({lang.get('level', 'N/A')})\\n\"\n",
    "            readable += \"\\n\"\n",
    "        if \"skills\" in portfolio:\n",
    "            readable += \"Comp√©tences techniques:\\n\"\n",
    "            readable += \", \".join(portfolio[\"skills\"]) + \"\\n\\n\"\n",
    "        if \"similarity_score\" in portfolio:\n",
    "            readable += f\"Score de similarit√©: {portfolio['similarity_score']}\\n\"\n",
    "    else:\n",
    "        # Sinon, on retourne le JSON format√© avec indentations\n",
    "        readable = json.dumps(data, indent=4, ensure_ascii=False)\n",
    "    return readable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132eb354",
   "metadata": {},
   "source": [
    "## 6. Calcul de Similarit√© entre comp√©tences\n",
    "\n",
    "La fonction ci-dessous calcule le score de similarit√© entre les comp√©tences d'un candidat et un ensemble id√©al.\n",
    "\n",
    "L'√©quation utilis√©e est :  \n",
    "$$\\text{Score} = \\frac{n}{N} \\times 100$$  \n",
    "\n",
    "o√π $n$ est le nombre de comp√©tences communes et $N$ le nombre total de comp√©tences id√©ales.\n",
    "\n",
    "**Toy Example :**\n",
    "\n",
    "```python\n",
    "score = compute_similarity({\"Python\", \"R\"}, {\"Python\", \"R\", \"SQL\"})\n",
    "# R√©sultat attendu : environ 66.67%\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaff74a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compute_similarity(candidate_skills, ideal_skills):\n",
    "    common_skills = candidate_skills.intersection(ideal_skills)\n",
    "    score = (len(common_skills) / len(ideal_skills)) * 100\n",
    "    return round(score, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f423ee40",
   "metadata": {},
   "source": [
    "## 7. Utilisation de Camembert pour la NER\n",
    "\n",
    "Cette section montre comment utiliser le mod√®le Camembert NER pour extraire des entit√©s du texte.\n",
    "\n",
    "Nous d√©finissons d'abord quelques listes pr√©d√©finies de comp√©tences, langues, soft skills, dipl√¥mes, secteurs et notions math√©matiques.\n",
    "\n",
    "Ensuite, la fonction `extract_ner` applique le mod√®le Camembert sur le texte (apr√®s normalisation avec `.title()`)\n",
    "et cat√©gorise chaque entit√© en fonction des listes.\n",
    "\n",
    "$$\\text{Camembert NER : } \\text{ Texte } \\to \\text{ Entit√©s cat√©goris√©es}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc70403",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Listes pr√©d√©finies\n",
    "competences = {\n",
    "    \"Python\", \"R\", \"Java\", \"C++\", \"SQL\", \"JavaScript\", \"Docker\", \"Kubernetes\", \"Excel\", \"Power BI\", \"Git\",\n",
    "    \"Tensorflow\", \"Pytorch\", \"Scikit-Learn\", \"Numpy\", \"Scipy\", \"Matplotlib\", \"Pandas\", \"AWS\", \"Linux\", \"TCP/IP\",\n",
    "    \"PostgreSQL\", \"Scrum\", \"JIRA\", \"CI/CD\", \"Machine Learning\", \"Deep Learning\", \"Reinforcement Learning\", \"NLP\", \"Computer Vision\",\n",
    "    \"MySQL\", \"MongoDB\", \"NoSQL\", \"Spark\", \"Hadoop\"\n",
    "}\n",
    "\n",
    "langues = { \"Fran√ßais\", \"Anglais\", \"Espagnol\", \"Allemand\", \"Chinois\", \"Russe\", \"Arabe\", \"Italien\", \"Portugais\"}\n",
    "\n",
    "soft_skills = {\n",
    "    \"Leadership\", \"Communication\", \"Esprit D'√©quipe\", \"Adaptabilit√©\", \"Cr√©ativit√©\",\n",
    "    \"Autonomie\", \"R√©solution De Probl√®mes\", \"Gestion Du Temps\", \"Esprit Analytique\", \n",
    "    \"Gestion Du Stress\", \"Organisation\", \"Initiative\", \"Planification\", \"Proactivit√©\",\n",
    "    \"Gestion De Projet\", \"Coordination\", \"Collaboration\", \"Gestion De Conflit\", \"Vulgarisation\",\n",
    "    \"Pr√©sentation Orale\", \"Discipline\", \"Empathie\", \"Tol√©rance\", \"Confiance En Soi\", \"Pers√©v√©rance\",\n",
    "    \"Curiosit√©\", \"Polyvalence\", \"M√©moire\", \"Esprit Critique\"\n",
    "}\n",
    "\n",
    "diplomes_certif = {\n",
    "    \"BTS\", \"DUT\", \"Licence\", \"Master\", \"Doctorat\", \"PhD\", \"MBA\", \n",
    "    \"TOEIC\", \"TOEFL\", \"IELTS\", \"Certifi√© AWS\", \"PMP\", \"Scrum Master\", \n",
    "    \"Cisco CCNA\", \"Microsoft Azure\", \"Google Cloud Professional\", \n",
    "    \"Coursera Deep Learning\", \"MITx Machine Learning\"\n",
    "}\n",
    "\n",
    "secteurs = {\n",
    "    \"Finance\", \"Banque\", \"Assurance\", \"Sant√©\", \"Pharmaceutique\", \"Biotechnologies\", \"A√©ronautique\", \"A√©rospatial\", \"D√©fense\",  \n",
    "    \"Automobile\", \"T√©l√©communications\", \"√ânergie\", \"Nucl√©aire\", \"√ânergies Renouvelables\", \"Transport\", \"Ferroviaire\", \"Maritime\",  \n",
    "    \"E-commerce\", \"Grande Distribution\", \"√âducation\", \"Cybers√©curit√©\", \"Automatisation\", \"Robotique\", \"Informatique\", \"Cloud Computing\", \n",
    "    \"Big Data\", \"Intelligence Artificielle\", \"Domotique\", \"Agroalimentaire\", \"Environnement\",  \n",
    "    \"Luxe\", \"Mode\", \"M√©dias\", \"Jeux Vid√©o\"\n",
    "}\n",
    "\n",
    "maths_algo = {\n",
    "    \"Statistiques\", \"Probabilit√©s\", \"Test D'Hypoth√®ses\", \"Estimation\", \"R√©gression\", \"S√©ries Temporelles\",  \n",
    "    \"Optimisation\", \"Programmation Lin√©aire\", \"Optimisation Combinatoire\", \"Programmation Dynamique\", \"Optimisation Convexe\",\n",
    "    \"Descente De Gradient\", \"Algorithmes D'Optimisation Stochastique\", \"Graphes\", \"R√©seaux\", \"Th√©orie Des Graphes\",  \n",
    "    \"Algorithmes De Tri\", \"Algorithmes Sur Les Graphes\", \"Programmation Parall√®le\", \"Analyse Num√©rique\", \"√âquations Diff√©rentielles\", \n",
    "    \"Approximation Num√©rique\", \"Interpolation\", \"Monte-Carlo\", \"Int√©gration Num√©rique\", \"Transform√©e De Fourier\", \"Processus Markoviens\",  \n",
    "    \"Algorithmes EM\", \"Traitement D'Images\", \"Analyse Topologique Des Donn√©es\", \"Cryptographie\", \"Th√©orie Des Nombres\"\n",
    "}\n",
    "\n",
    "def extract_ner(text):\n",
    "    # Charger le mod√®le Camembert NER\n",
    "    tokenizer = CamembertTokenizer.from_pretrained(\"Jean-Baptiste/camembert-ner\")\n",
    "    ner = pipeline(\"ner\", model=\"Jean-Baptiste/camembert-ner\", tokenizer=tokenizer, grouped_entities=True)\n",
    "    \n",
    "    # Normalisation du texte\n",
    "    text_capitalized = text.title()\n",
    "    \n",
    "    results = ner(text_capitalized)\n",
    "    \n",
    "    # Initialiser les listes pour chaque cat√©gorie\n",
    "    comp = []\n",
    "    lang_list = []\n",
    "    soft = []\n",
    "    dipl = []\n",
    "    sect = []\n",
    "    math = []\n",
    "    \n",
    "    # Parcourir les r√©sultats et cat√©goriser les entit√©s\n",
    "    for entity in results:\n",
    "        word = entity[\"word\"]\n",
    "        entity_type = entity[\"entity_group\"]\n",
    "        score = entity[\"score\"]\n",
    "        \n",
    "        if word in competences:\n",
    "            entity_type = \"COMPETENCE\"\n",
    "            comp.append(word)\n",
    "        if word in langues:\n",
    "            entity_type = \"LANGUE\"\n",
    "            lang_list.append(word)\n",
    "        if word in soft_skills:\n",
    "            entity_type = \"SOFT SKILL\"\n",
    "            soft.append(word)\n",
    "        if word in diplomes_certif:\n",
    "            entity_type = \"DIPLOME\"\n",
    "            dipl.append(word)\n",
    "        if word in secteurs:\n",
    "            entity_type = \"SECTEUR\"\n",
    "            sect.append(word)\n",
    "        if word in maths_algo:\n",
    "            entity_type = \"MATHS/ALGO\"\n",
    "            math.append(word)\n",
    "            \n",
    "        print(f\"{word} -> {entity_type} ({score:.2f})\")\n",
    "    \n",
    "    return {\n",
    "        \"competences\": comp,\n",
    "        \"langues\": lang_list,\n",
    "        \"soft_skills\": soft,\n",
    "        \"diplomes_certif\": dipl,\n",
    "        \"secteurs\": sect,\n",
    "        \"maths_algo\": math\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934a4ca4",
   "metadata": {},
   "source": [
    "**Toy Example 2 ‚Äì Extraction NER avec Camembert :**\n",
    "\n",
    "Nous utilisons un petit texte toy qui contient des comp√©tences connues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c24323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_text = \"Python and Machine Learning are key skills. I also know R and SQL.\"\n",
    "ner_toy = extract_ner(toy_text)\n",
    "print(\"R√©sultats NER (toy):\")\n",
    "print(ner_toy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5718e28",
   "metadata": {},
   "source": [
    "## 8. Tests et Exemples compl√©mentaires\n",
    "\n",
    "Voici quelques exemples suppl√©mentaires pour illustrer les fonctions qui ne n√©cessitent pas de fichiers externes.\n",
    "\n",
    "**Toy Example ‚Äì Calcul de similarit√© :**\n",
    "\n",
    "```python\n",
    "score = compute_similarity({\"Python\", \"R\"}, {\"Python\", \"R\", \"SQL\"})\n",
    "# R√©sultat attendu : environ 66.67%\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bb14c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_candidate_skills = {\"Python\", \"R\"}\n",
    "toy_ideal_skills = {\"Python\", \"R\", \"SQL\"}\n",
    "similarity_toy = compute_similarity(toy_candidate_skills, toy_ideal_skills)\n",
    "print(\"Score de similarit√© (toy) :\", similarity_toy, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e77766",
   "metadata": {},
   "source": [
    "**Toy Example ‚Äì Conversion JSON en format lisible :**\n",
    "\n",
    "Nous utilisons un JSON toy repr√©sentant un dossier de comp√©tences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d96ef9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "toy_json = \"\"\"\n",
    "{\n",
    "    \"skills_portfolio\": {\n",
    "        \"personal_info\": {\n",
    "            \"name\": \"Alice Dupont\",\n",
    "            \"residence\": \"Lyon, France\"\n",
    "        },\n",
    "        \"professional_experience\": [\n",
    "            {\n",
    "                \"duration\": \"2020-2022\",\n",
    "                \"company\": \"Startup X\",\n",
    "                \"context\": \"D√©veloppement d'applications Python\",\n",
    "                \"achievements\": \"Cr√©ation d'un prototype innovant\"\n",
    "            }\n",
    "        ],\n",
    "        \"education\": [\n",
    "            {\n",
    "                \"period\": \"2015-2018\",\n",
    "                \"degree\": \"Licence en Informatique\",\n",
    "                \"school\": \"Universit√© de Lyon\"\n",
    "            }\n",
    "        ],\n",
    "        \"languages\": [\n",
    "            {\"name\": \"French\", \"level\": \"Natif\"}\n",
    "        ],\n",
    "        \"skills\": [\"Python\", \"Machine Learning\"],\n",
    "        \"similarity_score\": \"75%\"\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "readable_toy = json_to_human_readable(toy_json)\n",
    "print(\"Conversion JSON (toy) -> Lisible:\")\n",
    "print(readable_toy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8fe52b",
   "metadata": {},
   "source": [
    "## 9. Fonction Main ‚Äì Pipeline complet\n",
    "\n",
    "La fonction principale int√®gre l'ensemble du pipeline et affiche les r√©sultats.\n",
    "\n",
    "**Remarque :** Pour les fonctions d'extraction de PDF (extraction_pipeline et extract_with_donut),\n",
    "un chemin vers un PDF r√©el est n√©cessaire. Les exemples toy pr√©sent√©s ci-dessus montrent les autres parties du pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b942d8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Chemin vers un PDF r√©el (√† adapter)\n",
    "    pdf_path = \"C:\\\\Users\\\\mathi\\\\Desktop\\\\Mathis_Chabaud_CV.pdf\"\n",
    "    \n",
    "    print(\"=== D√©but du pipeline d'extraction ===\")\n",
    "    \n",
    "    # Extraction via pipeline (texte natif ou OCR)\n",
    "    text_output = extraction_pipeline(pdf_path)\n",
    "    print(\"=== Texte extrait ===\")\n",
    "    print(text_output)\n",
    "    \n",
    "    # Extraction OCR-Free avec Donut\n",
    "    donut_result = extract_with_donut(pdf_path)\n",
    "    print(\"=== Donut Output ===\")\n",
    "    print(donut_result)\n",
    "    \n",
    "    # Conversion de la sortie JSON en format lisible\n",
    "    print(\"=== Conversion JSON -> Lisible ===\")\n",
    "    readable = json_to_human_readable(donut_result)\n",
    "    print(readable)\n",
    "    \n",
    "    # Extraction NER via Camembert sur le texte extrait\n",
    "    print(\"=== Extraction NER via Camembert ===\")\n",
    "    ner_res = extract_ner(text_output)\n",
    "    print(\"Entit√©s extraites:\")\n",
    "    print(ner_res)\n",
    "    \n",
    "    # Calcul de similarit√© (exemple)\n",
    "    ideal_skills = {\"Python\", \"R\", \"SQL\", \"Machine Learning\", \"Big Data\",\n",
    "                    \"Deep Learning\", \"Statistics\", \"Data Visualization\",\n",
    "                    \"Time Series Analysis\", \"Cloud Computing\", \"Docker\",\n",
    "                    \"Apache Spark\", \"Hadoop\"}\n",
    "    candidate_skills = {\"Python\", \"R\", \"SQL\", \"Machine Learning\"}\n",
    "    similarity = compute_similarity(candidate_skills, ideal_skills)\n",
    "    print(\"Score de similarit√© (exemple) :\", similarity, \"%\")\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
